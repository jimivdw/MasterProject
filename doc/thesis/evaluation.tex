\chapter{Evaluation}
\chlab{evaluation}

In order to determine which of the two implementations of \oframp{} is the best, and to evaluate the project in general, the system will be subjected to a user study. This chapter will discuss the project's hypotheses~(\secref{hypotheses}) and the contents of the user studies, including the user tasks~(\secref{us_tasks}) and questionnaires~(\secref{us_questionnaires}). Finally, this chapter will discuss how the experiment outcomes should be analysed, and what certain outcomes should mean~(\secref{us_analysis}).


\section{Hypotheses}
\seclab{hypotheses}
According to Jonassen's dichotomy of problems~\cite{jonassen2000toward}~(see also \secref{id_learning}), fragment-based molecule parameterisation is an ill-structured problem. This means that the problem is quite complex and is therefore hard to solve. Due to this, users need to be ensured that there is a way to find a solution, or they will simply give up. This should be the case for both versions of the system, but is better facilitated by the naive version than the smart one. In the naive version, users will see that, by selecting an atom, it can easily be parameterised. He will know that, in order to complete the parameterisation, this process simply needs to be repeated for the remainder of the atoms.

The differences between the two implementations, as discussed in \secref{id_comparison}, combined with the motivations behind the two interaction designs~(see \secref{id_versions}), can lead to the conclusion that the smart version has great theoretical benefits in time consumption and user guidance, where the naive version should excel in user freedom. In the earlier mentioned study of Payne, Sycara, and Lewis~\cite{payne2000varying}, this was the same. They concluded that many users found that using the naive version was a tedious process that cost a lot of time. However, in some, more complex cases, they found that the system users find it really important to have full control of the system, and therefore preferred the naive interaction design. As fragment-based molecule parameterisation is an ill-structured, and therefore complex problem, the following hypothesis about the two versions of the interaction design can be formulated:
\begin{quote}
The `naive' interaction design is the best design for a tool that is used for fragment-based molecule parameterisation.
\end{quote}

On a more general level, both versions of the interaction design rely on the idea that fragment-based molecule parameterisation should be chemically correct. As this has not been proven yet, \oframp{} aims to help in determining if it is possible. Both versions of the system have been designed in such way that the user should be able to quickly parameterise a molecule, either by providing them with a clear overview of possibilities, or by limiting the user interaction to accepting and rejecting. Furthermore, as it has been proven that properly matching parts of molecules have the same atomic charges, the second - for both versions of \oframp{} essential - hypothesis is as follows:
\begin{quote}
It is possible to parameterise a molecule based on fragments of other molecules; both correctly and in a reasonable amount of time.
\end{quote}


\section{User studies}
To evaluate the hypotheses discussed in the previous section, both versions of the developed system will be subjected to a number of user studies. This will help to determine if the system works properly and whether it is really fit for the molecule parameterisation task. Unfortunately, not just anyone can be asked to test \oframp, as it is a system that is aimed to be used by experienced chemists who understand the details about molecule parameterisation. Luckily, project supervisors Klau and El-Kebir keep a close connection with a substantial group of chemistry researchers, creating the opportunity of asking them to participate in a user study. They were interested in the concept of fragment-based molecule parameterisation, and willing to test a system that does that.

\subsection{Use cases}
\seclab{us_tasks}
In the user studies, the participants will be split up in two groups, where each person is randomly assigned to a group and both groups are of equal size. This way, it is made sure that both groups are representative, which is essential for user studies~\cite{wohlin2003empirical}~(see also \secref{user_studies}). The groups will take part in an experiment that compares the two versions of the molecule parameterisation tool. Due to the limited number of test subjects, every participant will be asked to test both the naive and the smart version of \oframp. As the effects of the order in which two systems are evaluated are often remarkable, one group will start the evaluation with the naive version, followed by the smart version, where the other group will do this the other way around.

For both versions of the tool, the experiment participants will be asked to complete the Demo mode first, such that they can get used to that version of the system. Next, they will be asked to parameterise a small, simple molecule, followed by a larger, more complex one. They are then asked to fill in a short questionnaire about that version of the system~(see \secref{us_questionnaires}), after which they are asked to switch to the other version of \oframp. When the steps for both versions have been completed, a final questionnaire, about \oframp{} in general, needs to be filled in. The complete set of instructions can be found in \appref{instructions}.

In order to be able to obtain results from the user studies, an extensive logging mechanism has been built into \oframp\footnote{This mechanism is only present in the special `experiment' version.}. It tries to log as much as it can; from browser details to system load times, and from mouse clicks to button presses. Every log message contains a time-stamp, such that the time difference between two log events can be calculated. They also include a message type, such that they can easily be filtered and counted. Finally, the resulting parameterisation is stored in the log as well, to be able to grade the user's performance.


\subsection{Questionnaires}
\seclab{us_questionnaires}
After completing their tasks on either of the two versions, the users will be asked to answer a number of questions about the tool. These will mainly be questions about how they like the design and if they can see themselves using it, but will also ask them for suggestions on things that can be improved or added. This way, their experiences can be used to further improve the system, and to make it into something they really like to use.

The questionnaire that test subjects will be asked to fill in will be based on the \verb|SUS| and \verb|UMUX-LITE| usability metrics~\cite{lewis2013umux}~(see also \secref{user_studies}). It will consist of the, for this tool, relevant questions from those metrics, followed by some questions about what they liked and disliked about the tool. To be certain that the questions are answered from the interaction design point of view, they will be asked from both the chemistry and interaction design perspectives. This gives the test subjects a place to put their comments on the chemical correctness, such that their feedback on the interaction design will truly concern the design. The complete set of questionnaire questions can be found in \appref{questionnaires}.

After testing both the naive and smart versions of \oframp, and assessing them with the previously discussed questionnaires, a few more, general questions will be asked. These questions are about what version the user liked best and why, if he wants to see any functionality added to the system, and leaves some room for additional comments. They will probably not provide new insights as to which version is liked better, as this can be inferred from the ratings obtained by the previous questionnaires, but they will help the further development of \oframp.

The questionnaires, just like the experiment, needed to be held online. Because of this, they have been implemented as web forms using (a minor extension of) the questionnaire language QL~\cite{erdweg2013state}. This allowed for easy definition of the form, without having to worry about validation or layout.

\subsection{Analysis}
\seclab{us_analysis}
The results from the log of the user studies combined with those obtained with the questionnaire should be able to give an answer on which of the two interaction designs is better. The version that gives good results in a short amount of time, and is also positively graded in the questionnaire, will be the better one. Furthermore, the timing and scoring results from the experiment alone will be able to answer the question if it is possible to do fragment-based molecule parameterisation at all. At least for one of the two versions both time consumption and parameterisation results should be reasonable.

In order to rate the user's parameterisation, the molecules that the user will be asked to parameterise should already have been parameterised using the conventional quantum mechanical calculations. This way, the user's results can be compared to the outcomes of the calculations to establish his rating. The smaller the difference between the two, the better the performance of the user will be graded.

There are two different ways of calculating this:
\begin{align*}
R &= |\Phi - C| & r &= \sum_{i = 1}^{n} |\delta_{i}|,
\end{align*}
\vspace{-1em}
\begin{align*}
\text{where} \quad \Phi &= \text{ATB total molecule charge},\\
C &= \text{\oframp{} total molecule charge},\\
n &= \text{total number of atoms in the molecule},\\
\delta_{i} &= \phi_{i} - c_{i},\\
\phi_{i} &= \text{ATB charge of the atom with index}~i,\\
c_{i} &= \text{\oframp{} charge of the atom with index}~i.
\end{align*}

First, it is interesting to know the difference in the total charge of the molecules, which is given by $R$. This is something that can be observed and minimised by the user, as he can see the totally assigned charge during the parameterisation process. It can therefore be used to see if the user is paying attention to that.

More important, but not less interesting, is the sum of the absolute differences between the two parameterisations, which is found by $r$. This will determine how good the results of \oframp{} really are, as it concerns the correctness of the individual atom charges.

Using the results obtained from the answers of the \verb|SUS| / \verb|UMUX| questionnaire, a final score can be calculated. This score is given by the following formula~\cite{sauro2011measuring}:
\[
s = 2.5 \times \sum_{i = 1}^{10} q_{i} - 1,
\]
where $q_{i}$ denotes the answer to question $i$.

In a study of over 500 outcomes of \verb|SUS| questionnaires, it has been found that the average system has a total score of $68$~\cite{sauro2011measuring}~(see also \secref{user_studies}). This means that, in order to be successful, at least one version of \oframp{} should score higher than that. Another study identified a way to translate \verb|SUS| scores to adjectives~\cite{bangor2009determining}. Systems with a \verb|SUS| score of below $20$ can be considered the `worst imaginable', a score of 48 to 65 is `OK', and 65 to 83 is `good'. The best version of the interaction design should therefore at least be rated as `good'.
