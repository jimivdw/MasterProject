\chapter{Evaluation}
\chlab{evaluation}

In order to determine which of the two implementations of \oframp{} is the best, and to evaluate the project in general, the system will be subjected to a user study. This chapter will discuss the project's hypotheses~(\secref{hypotheses}) and the contents of the user studies, including the user tasks~(\secref{us_tasks}) and questionnaires~(\secref{us_questionnaires}).


\section{Hypotheses}
\seclab{hypotheses}
According to Jonassen's dichotomy of problems~\cite{jonassen2000toward}~(see also \secref{id_learning}), fragment-based molecule parameterisation is an ill-structured problem. This means that the problem is quite complex and is therefore hard to solve. Due to this, users need to be ensured that there is a way to find a solution, or they will simply give up. Blah \ldots.
% Errrm...
% This is better facilitated by the smart version than the naive version of the tool, as one can fully parameterise a molecule just by clicking `yes' a number of times in that version. The solution will probably not be too good, but it is definitely a solution and users will continuously see there is a way to get there. This is something that is less clear in the naive version of the tool, where more user action is needed to get to the solution.

The differences between the two implementations, as discussed in \secref{id_comparison}, combined with the motivations behind the two interaction designs~(see \secref{id_versions}), one can conclude that the smart version has great benefits in time consumption and user guidance, where the naive version excels in user freedom. In the earlier mentioned study of Payne, Sycara, and Lewis~\cite{payne2000varying}, this was the same. They concluded that many users found that using the naive version was a tedious process, but \ldots. This leads to the following hypothesis for this research project:
\begin{quote}
The `naive' interaction design is the best design for fragment-based molecule parameterisation.
\end{quote}

The second, for this project essential hypothesis, is as follows:
\begin{quote}
It is possible to correctly parameterise a molecule based on fragments of other molecules in a reasonable amount of time.
\end{quote}
This hypothesis should hold for both implementations of the tool, as both have been designed to converge to a solution and to provide their information to its clearest and cleanest extent. Blah \ldots.


\section{User studies}
To evaluate the hypotheses discussed in the previous section, both versions of the developed system will be subjected to a number of user studies.
% Needs to be redone..
% Currently, however, the presumed user base is limited to only a small number of researchers. Luckily, project supervisors Klau and El-Kebir keep a close connection with most of those researchers, who work at VU University Amsterdam. They have shown some interest in the tool and are willing to participate in the user studies. Furthermore, as some of the researchers are also teaching, they can ask some of their students to evaluate the tool as well. It is not possible to ask random students, as they will lack domain knowledge, making their results unreliable~\cite{jonassen2000toward}. The researchers' students are considered to have enough domain knowledge to be able to correctly parameterise a molecule.

In the user studies, the test subjects will be split up in two groups, where each subject is randomly assigned to a group and both groups are of equal size. This way, it is made sure that both groups are representative, which is essential for user studies~\cite{wohlin2003empirical}~(see also \secref{user_studies}).

\subsection{Use cases}
\seclab{us_tasks}
The groups will take part in an experiment that compares the two versions of the molecule parameterisation tool. Due to the limited number of test subjects, every participant will be asked to test both the naive and the smart version of \oframp. As the effects of the order in which two systems are evaluated are often remarkable, one group will start the evaluation with the naive version, followed by the smart version, where the other group will do this the other way around.

The tasks are blah \ldots.
% Needs to be redone..
% For both groups, every member will be asked to parameterise a few molecules of increasing size and complexity. The first one will be quite easy to complete, but the last one should be of such complexity that, theoretically, using the tool is beneficial over using conventional quantum mechanical calculations. It is in these last situations that the tool should show its true value.

Everything will be logged, blah \ldots.
% Redo this...
% For the experiment, timing and scoring mechanisms will be built into the tool. The timing mechanism will simply measure the time between the first click on the tool's ``Find matches'' button and the moment where the user saves the final parameterisation. This value will be stored in a database, along with the value obtained by the scoring mechanism. This value, as discussed before, will be calculated from the difference between the assigned charges and the previously calculated ones.
% In order to be able to explain the scoring results and to evaluate them in more detail, the proposed fragments and the selected ones will also be stored. This way, it should be possible to pinpoint where a user went wrong, if he did. Furthermore, it can also help to identify outliers. If it is clear that someone was just messing around with the tool and did not care about getting a good result, that user's experiment results can be left out.

During the tasks, the time required to complete the parameterisation will be measured. The difference between the time required in the two implementations will help deciding which of the two implementations is better, but, for both tools, completing the parameterisation should definitely not take hours to finish.

Besides time, the user will also be scored on his performance. In order to do this, the molecules that the user will be asked to parameterise should already have been parameterised using the conventional quantum mechanical calculations. This way, the user's results can be compared to the outcomes of the calculations to establish his rating. The smaller the difference between the two, the better the performance of the user will be graded. There are two different ways of calculating this:
\begin{align*}
r &= \sum_{i = 0}^{n} \delta_{i} & R &= \sum_{i = 0}^{n} |\delta_{i}|,
\end{align*}
\vspace{-1em}
\begin{align*}
\text{where} \quad \delta_{i} &= C_{i} - c_{i},\\
C_{i} &= \text{ATB charge of the atom with index}~i,\\
c_{i} &= \text{\oframp{} charge of the atom with index}~i,\\
n &= \text{total number of atoms in the molecule}.
\end{align*}

First, it is interesting to know the difference in the total charge of the molecules, which is given by $r$. This is something that can be observed and minimised by the user, as he can see the totally assigned charge during the parameterisation process, and can therefore be used to see if the user is paying attention to that. More important, but not less interesting, is the sum of the absolute differences between the two parameterisations, which is found by $R$. This will determine how good the results of \oframp{} really are, as it concerns the correctness of the individual atom charges.


\subsection{Questionnaires}
\seclab{us_questionnaires}
After completing their tasks on either of the two versions, the users will be asked to answer a number of questions about the tool. These will mainly be questions about how they like the design and if they can see themselves using it, but will also ask them for suggestions on things that can be improved or added. This way, their experiences can be used to further improve the system, and to make it into something they really like to use.

The questionnaire that test subjects will be asked to fill in will be based on the \verb|SUS| and \verb|UMUX-LITE| usability metrics~\cite{lewis2013umux}~(see also \secref{user_studies}). It will consist of the, for this tool, relevant questions from those metrics, followed by some questions about what they liked and disliked about the tool. To be certain that the questions are answered from the interaction design point of view, they will be asked from both the chemistry and interaction design perspectives. This gives the test subjects a place to put their comments on the chemical correctness, such that their feedback on the interaction design will truly concern the design.

Final questionnaire blah \ldots.

Implemented using QL~\cite{erdweg2013state}\ldots

\subsection{Analysis}
\seclab{us_analysis}
How to analyse the results? Blah \ldots.
% Redo, or maybe even remove...
% The results of the user studies combined with those obtained with the questionnaire should be able to give an answer on which of the two interaction designs is better. The version that gives good results in a short amount of time and is also positively graded in the questionnaire will be the better one. Furthermore, the timing and scoring results from the experiment alone will be able to answer the question if it is possible to do fragment-based molecule parameterisation at all. At least for one of the two versions both time consumption and parameterisation results should be reasonable.
% Furthermore, users should not report they are annoyed by the tool or willing to stop parameterising.
