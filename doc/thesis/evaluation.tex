\chapter{Evaluation}
\chlab{evaluation}

Here will be the description of the project evaluation\ldots

\nlipsum


\begin{comment}
To evaluate this project's hypotheses~(see \secref{hypotheses}), both versions of the developed tool will be subjected to a number of user studies. Currently, however, the presumed user base is limited to only a small number of researchers. Luckily, project supervisors Klau and El-Kebir keep a close connection with most of those researchers, who work at VU University Amsterdam. They have shown some interest in the tool and are willing to participate in the user studies. Furthermore, as some of the researchers are also teaching, they can ask some of their students to evaluate the tool as well. It is not possible to ask random students, as they will lack domain knowledge, making their results unreliable~\cite{jonassen2000toward}. The researchers' students are considered to have enough domain knowledge to be able to correctly parameterise a molecule.

In the user studies, the test subjects will be split up in two groups, where each subject is randomly assigned to a group and both groups are of equal size and equal student-scientist ratio. This way, it is made sure that both groups are representative, which is essential for user studies~\cite{wohlin2003empirical}~(see also \secref{user_studies}).

The groups will take part in an experiment that compares the two versions of the molecule parameterisation tool~\cite{wohlin2003empirical}. One group will test the naive version, the other will use the smart one. For both groups, every member will be asked to parameterise a few molecules of increasing size and complexity. The first one will be quite easy to complete, but the last one should be of such complexity that, theoretically, using the tool is beneficial over using conventional quantum mechanical calculations. It is in these last situations that the tool should show its true value.

During the tasks, the time required to complete the parameterisation will be measured. The difference between the time required in the two implementations will help deciding which of the two implementations is better, but, for both tools, completing the parameterisation should definitely not take hours to finish. Furthermore, users should not report they are annoyed by the tool or willing to stop parameterising.

Besides time, the user will also be scored on his performance. In order to do this, the molecules that the user will be asked to parameterise should already have been parameterised using the conventional quantum mechanical calculations. This way, the user's results can be compared to the outcomes of the calculations to establish his score. The smaller the difference between the two, the better the performance of the user will be graded. Of course, there will always be a small difference between the manual and calculated ways, as the manual assignment cannot be as precise as the calculated one is. However, as long as the difference is small, the developed tool can be considered useful to speed up atomic charge assignment.

For the experiment, timing and scoring mechanisms will be built into the tool. The timing mechanism will simply measure the time between the first click on the tool's ``Find matches'' button and the moment where the user saves the final parameterisation. This value will be stored in a database, along with the value obtained by the scoring mechanism. This value, as discussed before, will be calculated from the difference between the assigned charges and the previously calculated ones.

In order to be able to explain the scoring results and to evaluate them in more detail, the proposed fragments and the selected ones will also be stored. This way, it should be possible to pinpoint where a user went wrong, if he did. Furthermore, it can also help to identify outliers. If it is clear that someone was just messing around with the tool and did not care about getting a good result, that user's experiment results can be left out.

After completing their tasks, the users will be asked to answer a number of questions about the tool. These will mainly be about how they like the design and if they can see themselves using it, but will also ask them for suggestions on things that can be improved or added. This way, their experiences can be used to further improve the tool, and to make it a tool they really like using.

The questionnaire that test subjects will be asked to fill in will be based on the \verb|SUS| and \verb|UMUX-LITE| usability metrics~\cite{lewis2013umux}~(see also \secref{user_studies}). It will consist of the, for this tool, relevant questions from those metrics, along with some additional application-specific questions. It will be made sure the questions follow some natural flow, in order to encourage the testers to report their detailed experiences~\cite{tuch2013analyzing}. Furthermore, the questions will be somewhat directive, to reduce the chances of test subjects forgetting to report certain things.

The results of the user studies combined with those obtained with the questionnaire should be able to give an answer on which of the two interaction designs is better. The version that gives good results in a short amount of time and is also positively graded in the questionnaire will be the better one. Furthermore, the timing and scoring results from the experiment alone will be able to answer the question if it is possible to do fragment-based molecule parameterisation at all. At least for one of the two versions both time consumption and parameterisation results should be reasonable.
\end{comment}