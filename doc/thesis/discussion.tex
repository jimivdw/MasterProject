\chapter{Discussion}
\chlab{discussion}

From the results obtained in the user studies, it can be decided which version of \oframp{} is the best for the task of fragment-based molecule parameterisation. What version that is will be argued in this chapter, along with a description of how it can be improved in the future. Additionally, some threats to the validity of the experiment outcomes will be discussed.


\section{The best version}
There are several factors that can help to determine which interaction design for \oframp{} is the best. First, the time required to complete a parameterisation an be considered. As discussed in \secref{res_time}, the smart version requires the least time per atom, and will therefore be the fastest to completely parameterise a molecule. With a total median time of under five minutes for parameterising all four molecules in the user study, it can also be considered to be fast in general. Time-wise, the smart version is therefore the best.

Second, one can look at the correctness of the resulting parameterisations of the different versions. The results presented in \secref{res_rating} show that the naive version has the lowest overall charge difference for both the first and second set of molecules. Furthermore, it has a lower average atom charge difference for every parameterised molecule as well, with average differences of $0.054$ for the smaller molecules, and $0.146$ for the bigger ones. This means that, result-wise, the naive version is the best.

The final information that can be gathered from the user action log is the number of actions that needed to be undone, and the total number of clicks that needed to be made to complete the parameterisations. As was described in \secref{res_other}, the naive version required almost no undo commands, while the smart version required the use of many. Furthermore, the total number of clicks was also smaller for the naive version, making this the better version in those two aspects as well.

Besides the action logs, the \verb|SUS| ratings of the two versions of the system can also be used to decide which one is best. As discussed in \secref{res_sus}, the naive version scored higher than or equal to the smart version for all but one statement. Additionally, with a total \verb|SUS| score of $76$, it not only had a higher score than the smart variant, but was also graded above average, or `good'.

Comments the user studies participants made on the smart version were less positive than those made on the naive version. This also showed in their preference verdict, where all participants voted for the naive version, rather than the smart variant~(see \secref{res_preference}). Most of them commented they strongly preferred that version.

Combining all of this data, it is clear that the naive interaction design is the better of the two. It gives better results, and is found to be better fit for molecule parameterisation than the smart design. Even though it takes slightly more time to complete a parameterisation in that version, it still is reasonably fast. However, it may not be considered the perfect interaction design yet. Many suggestions on how it could be improved have been made~(see \secref{res_suggestions}), resulting in some future work which will be discussed later in this chapter.


\section{Threats to validity}
As in every research project, there are always a few threats to the validity of its outcomes. For the conclusions on the best interaction design for \oframp, this is no different. Most important here is the limited number of participants in the user studies. Due to the limited amount of people currently involved in manual molecule parameterisation, and the limited amount of time in which they had to complete the experiment~(one week), only 10 people were able to partake. However, it has been shown that a sample size of only 10 people leads to a correct \verb|SUS| conclusion $80\%$ of the time~\cite{tullis2004comparison}.

This limited amount of experiment participants also lead to the decision that every user would have to test both versions of the system. While this is not a bad thing in general, it would have been preferred to do a double-blind test to make sure users were not primed by the first version they used. It has been attempted to mitigate this risk by dividing the participants over two groups, where both would use the systems in a different order, but this does not rule out any effect it may have had on the results.

The fact that users had to test the two versions of the system unfortunately also doubled the time they needed to complete the experiment. Even though the total time it took to complete all parameterisations was under 10 minutes on average, users also had to complete two different demo modes and fill in three questionnaires. This brings the total time for the average user to around 45 minutes, where in extreme cases users took far over an hour from loading the first version to submitting the final questionnaire. This may potentially have lead to loss of concentration, and might have incited users to take shortcuts in the parameterisation process, although this has not explicitly been observed.

By organising the user studies over the internet, there has been no control of the environment in which the participants performed the experiment. Although the users were instructed to make a large time slot available for it, and asked to try to focus as much as they could, it may be possible that they got interrupted at certain points, or used the system in a busy environment. This, unfortunately, is one of the insuperable side-effects of conducting an on-line user study.

Another possible threat to the validity of the results is the choice of atoms for the user study. For the experiment, four molecules (two small, two large) were randomly selected from the \verb|ATB|. It is unknown whether these were good example molecules or not, as they may have some special properties, or similar molecules may not exist at all.

Additionally, as commented by many users, the fragment finding system is not perfect yet. It does sometimes find matches that do not make a lot of sense from a chemistry perspective, and the rating system needs to be improved such that the best matches will in fact also be the highest rated. Even though the development of the fragment finding system was outside the scope of this project, it can still have had some influence on the outcomes.

In order to find the best interaction design for a fragment-based molecule parameterisation system, two different designs have been implemented and compared. Although there was a strong preference for the naive version, it may be the case that a third, completely different system would be even better. However, even though the users had many suggestions on how the system could be improved further, none of these suggestions was really ground-breaking from the interaction design point of view. This does not rule out the possibility of the existence of an even better version, but does reduce the chances for it.

Finally, an unfortunate implementation/design error in the system that stored the user comments lead to the loss of the comments of two users. Although their \verb|SUS| ratings did not really deviate from the others, it may be the case that they made some different comments, or actually picked the smart version as their preferred one. Sadly, there is no way to recover these comments, but the similarity of the \verb|SUS| results suggests that there was no big difference of these users' opinion.


\section{Future work}
\nlipsum
