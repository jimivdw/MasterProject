\chapter{Related work}
\chlab{relwork}

Quite a lot of literature is available that relates to interaction design for systems like the tool for fragment-based molecule parameterisation that is discussed in this thesis. The literature has been divided into four different categories, each of which discusses a number of papers. Additionally, supporting literature for the work discussed here can be found in \appref{relwork_extra}.


\section{Molecule parameterisation}
\seclab{simulations}
For running molecular simulations, a force-field model describing a molecule's interatomic interactions is required~\cite{canzar2012charge}. In the simulations, the force field requires a specific topology, which includes the molecule's atom types, bonds, bond angles and atomic charges. Using this information, a molecule can be divided into a set of non-overlapping charge groups: sets of \emph{connected} atoms each of which' charge is ideally equal to the molecule's total charge. The work of Canzar, El-Kebir, et al. shows the need for finding the atomic partial charges of a molecule, as these are required for finding the charge groups, which in turn are essential for running molecule simulations.

Malde et al. present the Automated Force Field Topology Builder~(ATB)~\cite{malde2011automated}. The ATB is a web service that can provide topologies that can be used in molecular simulations. It can both act as a repository for molecules that have already been parameterised, and is able to parameterise molecules itself. This automatic parameterisation involves quantum mechanical calculations, which are very complex and computationally intensive. As it is believed that there is no way to speed up these calculations, a new approach is needed, using which atomic charges \emph{can} be found quickly. The topologies that are currently present in the ATB can be used to verify the new approach, by comparing the charges found with the new approach to the charges that are present in the ATB topologies.


\section{Interaction design}
\seclab{design}
% Besides trying to optimise the preparation steps for molecule simulations, the main challenge of this thesis project is to come up with a good interaction design for a tool that does that. In the interaction design area, Donald Norman is a respected and often cited author. He feels that technology can only live up to its full potential by, first of all, supporting human tasks, while making the supporting technology as transparent as possible~\cite{norman2002design}. This can be achieved by making the tools easy to use, easy to learn and easy to understand. Designing for this purpose is called user-centred design.

One of the most important aspects of design is visibility~\cite{norman2002design}. Every interface should have visible features that can send the right messages to the user. It is very important that user actions do not have coincidental consequences. Otherwise, the user can develop wrong expectations of his actions, which may later result in problems while using the tool. What is also important, is that if a user \emph{does} make a mistake, he should be able to undo this. When this is \emph{not} possible, he may get easily frustrated and will stop using the tool. Additionally, the interface should not get in the way of the task that needs to be performed, leading to the user spending his time `using the interface', but should rather help him executing his tasks~\cite{norman1990interfaces}. Finally, the user should not get lost in an giant list of features. As many of these features will often rarely be used, tools with less features, or where the advanced features are presented in a non-obtrusive way, are often the better ones.

When doing interaction design, one has to keep in mind that the appearance of a tool can have great impact on how well the user of that tool can carry out the tasks the tool is intended for~\cite{norman2002emotion}. Tools that are unattractive tend to focus the mind, which leads to better concentration. For tasks where something needs to be done quickly, this is good, but in cases where creative thinking is required, this will not provide a satisfying result. In that case, the thought processes should be broadened by having something that looks really good, thereby slightly distracting the user and allowing him to use creative thinking.

Not only the appearance of software tools is important, also the form in which the information it comprises is presented can make a big difference. For certain types of data, it is highly beneficial to be represented in a visual form. This allows the viewers to perceive patterns and relationships that might be missed in tables and numbers~\cite{gallopoulos1994computer}. Providing users with the ability to modify view parameters at runtime can increase the benefits even further, as they then have the freedom to explore every aspect of the data. However, as discussed before, these options can cause the application user to get lost in the list of options, no longer being able to execute the task he wants to perform. Therefore, the options should be located in some initially closed menu.

% Despite the fact that the importance of design has been long known, many software products are still poorly designed. In an attempt to overcome this, an approach called human-centred design (HCD) has been developed~\cite{norman2005human}. However, even though the needs of the user play a central role here, many companies following the HCD principles still develop complex and confusing products. The systems are often superb at the level of the static, individual display, but fail to support the sequential requirements of the underlying tasks and activities. Therefore, a new approach is needed that does not put the user in a central role, but rather does so with the activity that needs to be performed. In this Activity-Centred Design~(ACD) method, it is sometimes necessary to ignore a user's requests, as they might compromise the task that has to be carried out.

% This, however, does not mean that the application user's needs should be completely ignored. On the contrary, it is really important that the user's characteristics are understood and used in the design of an application~\cite{badre2002shaping}. Also, a good analysis of the context is needed to make the right design decisions. If one ignores the user characteristics and context of an application, the application is  likely to fail, as the context in which the application is developed will almost always differ from the context in which it will be used.

% \subsection{Learning}
% \seclab{id_learning}
% In order to determine what makes a good interaction design, it needs to be known how people learn new things and how our problem solving system works. There are two main learning mechanisms: schema acquisition, where knowledge of subject matter is organized in schemas that determine how new information is dealt with, and the transfer of learned procedures from controlled to automatic processing~\cite{sweller1994cognitive}. Well learned material can be processed automatically, without much effort, while new or less familiar information needs to be processed in a controlled fashion. This means that, when learning something new, one can only use that knowledge by devoting considerable cognitive effort to it. After getting more familiar with the task, the skill may become automatic. Only then can intellectual performance attain its full potential.

% In order to facilitate learning, the process of schema acquisition needs to be triggered and supported. Trying to teach by only providing examples will eventually even have the opposite effect. The learner will then be able to execute the task under the conditions that are equal to those in which he learned the skills, but will fail to do so in other cases. In order to overcome this, teaching should be done using goal-free problems. These problems focus on achieving various problem states and the steps on how to get there, which is exactly how schema acquisition works.

% How easy it is to learn something new is highly dependent on the level of interactivity between the different elements of a task. If there is no interaction between two elements, they can be learned in isolation, requiring less cognitive load than when they have to be learned simultaneously. However, making something easy to learn is not as simple as reducing the element interactivity, as it is dependent on the knowledge of the individual what constitutes an element. For the more experienced, an element may be a whole chunk of the information, while for the rest it may just be an atomic part of the information. Luckily, it is only hard to learn something new when both the intrinsic and extraneous cognitive loads are high. When there is only one high load, the lack of the other compensates for this.

% There are two types of problems: well-structured problems and ill-structured problems. In a well-structured problem, the initial state is well-defined, the goal state is known, and there is a limited set of logical operations to solve the problem~\cite{jonassen2000toward}. If either of these is missing, the problem is classified as being ill-structured. Ill-structured problems are generally more complex than well-structured problems, and are therefore often harder to solve.

% Besides the complexity, there are also many individual differences that determine how easy it is to solve a problem. First of all, when a person is familiar with the domain of a problem, it will be easier to solve for him than it will be for someone with less domain knowledge. Second, more analytical thinkers are often good problem solvers. They must believe that there will be a way to solve the problem, especially when the problem is ill-structured, as they might otherwise just give up before finding a solution. It also helps when someone has a certain affection with the problem. He will then be more likely to try and find the solution. If, however, one has certain beliefs or attitudes about a problem and its solution, he might be a less effective problem solver due to over-relying on that solution. Finally, every problem is different and requires different skills to solve. It can be easier to solve the problem for someone who is familiar with the problem type, but one may never forget that every problem requires a different way of learning.

\subsection[Principles]{Interaction design principles}
\seclab{id_principles}
Following the previously discussed and other research, a number of principles of interaction design can be identified. %However, different authors have different opinions on what those principles are.
Norman and Nielsen identify the following principles of interaction design that are completely independent of technology~\cite{norman2010gestural}:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item Visibility (also called perceived affordances or signifiers);
\item Feedback;
\item Consistency (also known as standards);
\item Non-destructive operations (hence the importance of undo);
\item Discoverability: all operations can be discovered by systematic exploration of menus;
\item Scalability: the operation should work on all screen sizes, small and large;
\item Reliability: operations should work and events should not happen randomly.
\end{itemize}
% Thimbleby, on the other hand, \emph{does} relate his principles to technology~\cite{thimbleby2007press}:
% \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
% \item Use good algorithms for better user interfaces: interactive devices are designed to solve problems in a structured way, which is exactly what a good algorithm does;
% \item Use simple, explicit interaction frameworks: those provide clear interaction structures, which allow for reliable feature integration, checking, analysis, fault identification, and error fixing;
% \item Interrelate all interaction programming: all aspects of the design should come out of the same specification;
% \item Define properties and analyse designs for them.
% \end{itemize}
% A more extensive list is provided by Blair-Early and Zender~\cite{blair2008user}:
% \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
% \item Obvious start: design an obvious starting point;
% \item Clear reverse: design an obvious exit or stop;
% \item Consistent logic: design an internally consistent logic for content, actions, and effects (the most important consistency is that with user expectations);
% \item Observe conventions: identify and consider the impact of familiar interface conventions;
% \item Feedback: design tangible responses to apt user actions;
% \item Landmarks: design landmarks as a reference for context;
% \item Proximity: design interface elements in consistent proximity to their content objects and to each other;
% \item Adaptation: design an interface that adapts or is adapted to use;
% \item Help: as necessary, provide a readily accessible overall mechanism for assistance;
% \item Interface is content: design interface elements that minimise interface and maximise content.
% \end{itemize}

% The above lists of principles are all quite different, but all consider automation of certain processes. The next section will discuss this in more detail.

\subsection{Automation}
\seclab{id_automation}
Software and artificial intelligence developments allow for increasing possibilities in automating processes. The amount of interaction between humans and computers can be reduced, allowing humans to concentrate on other tasks~\cite{payne2000varying}. However, it depends on the situation when automation should be applied and to what extent. Automation reduces people's situational awareness, which may be undesirable in some environments. Furthermore, it may leave users feeling out of control or can lead to deskilling if decisions are being made autonomously, rather than having a system that is only giving advise.

In order to determine what level of interaction is suitable for a task, multiple interaction methods need to be designed. These often include a `naive' version that only does validation of user input, a `cooperative' version in which the user is given advise on what to do, and a mostly automatic `autonomous' version that only requires some initial parameterisation~\cite{payne2000varying,horvitz1999principles}. In a case study on route planning agents it was found that, for that purpose, the cooperative version delivered the best results~\cite{payne2000varying}. The users of the autonomous version complained they lacked the possibility to fully control the system, while those who used the naive version complained the route planning process was tedious. However, this does not mean that the cooperative version is the silver bullet for man-machine interaction. In some cases, having full control might be necessary, or in other cases, where time consumption is really important, an autonomous version might be preferred.

Cooperative or mixed-initiative user interfaces have been studied in more detail by Horvitz~\cite{horvitz1999principles}. He has found that, in order for automation to work, automated activity should not occur before a user is ready for it. Delays in automation, on the other hand, can diminish its value, so it needs to be carefully timed. Among the most important factors for successful semi-automatic applications are: considering the computer's uncertainty about a user's goals, inferring the ideal action in light of costs, benefits and uncertainties, minimizing the cost of poor guesses, and providing mechanisms for efficient agent-user collaboration to refine results.

An example of a system that has implemented this cooperative interaction design is \verb|SALT|~\cite{marcus1987taking}, a tool for generating expert systems that are to be used in problem solving. Using \verb|SALT|, one will incrementally construct a design by either accepting or rejecting proposed design parameters, one parameter at a time. Constraints will constantly be checked, and, once a constraint violation is detected, the system will try to automatically remedy this. As it is extremely difficult to develop a system that has to perform a task for which expert knowledge is required, \verb|SALT| offers ways to backtrack the assigned parameters. It automatically detects parameters that might need to be modified, but require additional domain knowledge to be fixed properly.

At every step in \verb|SALT|'s incremental parameter assignment, the proposed values are provided in such order that the ones having the least negative effect come first. However, as this does not consider future assignments of other parameters, this means that the assignment may not always converge to a proper solution. This is where the previously discussed backtracking will be useful to see where the `wrong' decision has been made. In two field studies, \verb|SALT| has been proven a good and useful tool for problem solving. However, it has also been found that the required degree of interaction can vary among different systems. Therefore, one should always study the context of a system before deciding on the level of interaction that system will have.

% Automation is often said to be causing problems and to increase the chances for human error when failures occur. Norman proposes in~\cite{norman1990problem} that this is not the cause of automation itself, but rather of the automated systems' lack of appropriate feedback when humans need to take over control. In regular conditions and some predefined exceptional situations, automated systems are perfectly able to operate. It is in those other abnormal situations that automated systems often fail; both in executing their task and alerting the user that something unusual is happening. In order to overcome this, automated systems should be designed while keeping in mind that errors will occur. An appropriate feedback system needs to be present that allows for human intervention when needed.

% An often used, but wrong way of alerting a system's user that something is wrong, is the use of alarms for every component of the system~\cite{norman1990problem}. In exceptional situations, multiple components will often fail at the same time. When all of these would sound an alarm, the user could get confused and will not know what component requires immediate attention and what can wait. Rather than this, systems should continually inform the user about their state, such that he can detect exceptional situations and act upon them. Still, this information should be non-intrusive and presented in a natural way, such that man and machine can jointly solve the problem at hand.

\subsection{Abstraction}
\seclab{id_abstraction}
In order to be able to properly discuss an interaction design, this design needs to be represented at an abstract level. In~\cite{brehmer2013multi}, Brehmer and Munzner describe a multilevel typology for visualisation tasks. This typology consists of the answers to three questions: \verb|why| the task is performed, \verb|how| the task is performed and \verb|what| it pertains to. A list of predefined nodes is provided with which those questions must be answered. By comparing multiple of these typologies, one is able to reason about the differences between two systems on an abstract level, which can be used to hypothesise what system will work better under which circumstances.


\section{Molecule software}
\seclab{software}
There is a lot of software available for displaying, drawing and editing molecules. These programs form an indispensable part of every molecular processing system~\cite{ertl2010molecular}. Throughout the years, molecule software has evolved from basic text editors, through clickable image maps, to full-on molecular structure sketching software. In the last few years, a new trend becomes visible. More and more cheminformatics applications are being brought to the web, allowing for many new ways of interaction. Furthermore, these new applications are being open sourced, allowing for new innovative variations or combinations of the old tools.

An example of a previously offline, closed source tool is \verb|JSmol|, originally known as just \verb|Jmol|~\cite{hanson2013jsmol}. This tool has been seamlessly transformed from a \verb|Java| applet to \verb|JavaScript|, without any visible visual difference. Furthermore, performance wise, there is only a minor difference between the two implementations. Another example is \verb|JSME|~\cite{bienfait2013jsme}, a tool that has been cross-compiled from its original \verb|Java| code to \verb|JavaScript| using the \verb|Google Web Toolkit| compiler. The transition to the web has resulted in the addition of several features, suggested by users from the new, bigger audience. What can be concluded from these two cases is that bringing molecule software to the web opens up a whole world of new possibilities, without having to give up on performance.

With the ongoing migration of molecule software to the web, new devices, such as smartphones and tablets, will be able to run the tools. As these devices promote different ways of user interaction and often have smaller screens, designs of the molecule software need to be reconsidered. \verb|TB Mobile| is a mobile app for identification of potential anti-tuberculosis molecules~\cite{ekins2013tb}. The app is structured in such way that there is always a small control bar at the top, and a large area for showing the content. Interaction with the app is handled by a small number of large buttons, allowing for easy touch controls. In order to work for different screen sizes, everything in the content area is scalable, and, in case not everything fits on the screen, scrollable. Usage of the app in practice has shown that it helps to improve the work flow of tuberculosis researchers, by lowering the barriers for accessing the information it provides.

The issue of showing a lot of data on a limited-size screen is also addressed by Ertl and Rohde~\cite{ertl2012molecule}. They took the concept of a word cloud, and transformed that to the so called Molecule Cloud. Here the highest scored molecules are the biggest and the lowest are the smallest, just like the highest and lowest scored words in a word cloud. None of the molecules overlap, and the sizes are divided such that the whole available screen space is filled. User studies have shown that these molecule clouds provide an easy way of finding the most relevant molecules. However, when many molecules need to be displayed in a small area, this means that either all molecules will be very small, or the lowest scored molecules need to be left out completely. Depending on the situation, this may not be desirable.

\subsection{Molecule visualisation}
\seclab{ms_visualisation}
An important component of molecule software is the visualisation of the molecules. Molecules can be represented in a textual format, but it is often beneficial to use a visual representation, as this can help to illustrate various chemistry phenomena, such as the atomic structure. A study on Finnish upper secondary schools has shown that molecular modelling can truly be valuable~\cite{aksela2008computer}. The fact that students could interact with the models made it easier for them to quickly understand new chemical phenomena, and increased their interest in the subject.

% Molecule visualisation is generally done using either two-dimensional schematic views, or three-dimensional structure models. Which of these forms is best depends on the application for which the visualisation needs to be used. In cases where correct perception of the molecule structure including bond angles, bond lengths, and atom sizes is needed, 3D representations need to be used. A study on Slovenian primary and secondary schools has shown that, for understanding chemical concepts, 3D models are highly beneficial if they can be interacted with~\cite{ferk2003students}. However, as they convey more information than the 2D representations, it does take more time for the students to understand the 3D models. Furthermore, as the structures often have to be converted to a 2D form which the students can draw in their notebooks, transition rules need to be taught, which increases the steepness of the learning curve even further.

In cases where detailed molecule information needs to be described in an abstract way, two-dimensional schematic views, as opposed to three-dimensional structure models, are usually the most useful~\cite{zhou2009molecular}. In a 2D view, one can easily view every atom of a molecule, without having to rotate it. Furthermore, it can easily show which atoms are connected and what type of bond connects them. This creates a good overview of the molecule, and allows for easy comparison of two molecules.

For digitally visualising a molecule, a textual representation of it is needed that can be interpreted by a computer and transformed to a form that includes positional data. There are many different textual molecule formats, including \verb|SMILES|~\cite{daylight1992daylight}, \verb|InChI|~\cite{heller2013inchi}, \verb|Mol2|~\cite{tripos2005tripos}, and \verb|PDB|~\cite{bernstein1977protein}. Each of these formats serves its own purpose. \verb|SMILES| is mainly concerned with expressing atom types and bonds, \verb|InChI| adds charge information, \verb|Mol2| contains information about 2D or 3D atom positions, and \verb|PDB| files contain a lot of additional information about the proteins they describe.

% When a molecule in a textual format that lacks positional information needs to be visualised, its atoms' positions need to be calculated. Starting from the 1970s, algorithms for schematically displaying molecules in 2D have been developed. Dittmar, Mockus and Couvreur created one of the first programs that converted a connection table to a structural molecule diagram~\cite{dittmar1977algorithmic}. Back in the day, computing power was limited, which meant that advanced operations to eliminate overlap in the visualisation needed to be limited. Nevertheless, computing the diagrams still took over three minutes for an average sized molecule. Since then, advances in computer hardware and algorithm improvements have made it possible to speed up the structural diagram generation to times of under a second~\cite{fricker2004automated}. The improvements of computer hardware also allowed for better overlap prevention in the diagrams, leading to much cleaner visualisations~\cite{clark2006structure}. However, overlap is still not completely eliminated, so there is a need for further improvements in these algorithms.

In order to find the positional data of a molecule, it is possible to implement a position calculation algorithm, or use an existing system for this. Alternatively, as data formats including the positional data also exist, it is also possible to obtain this data by converting the input file to a format that does include it. Several systems exist that do this, including \verb|ChemAxon Molconvert|~\cite{chemaxon2014molecule} and \verb|Open Babel|~\cite{oboyle2011open}. \verb|Molconvert| is a closed-source, commercial program that is part of the \verb|ChemAxon Marvin| package. It allows for conversion of most accepted formats, but can only be used in production after buying a license. \verb|Open Babel| on the other hand is an open-source, free to use chemical toolbox. It supports even more formats than \verb|Molconvert|, but its atom position calculations are slightly less accurate.

As a molecule can essentially be seen as a graph of bounded degree with labelled nodes and edges, certain aspects of graph visualisation can be used when drawing molecules~\cite{boissonnat2001structure}. For very large molecules, graph reduction algorithms may provide a way to fit the whole molecule on a limited size screen~\cite{batagelj2004pajek}. Nodes of that reduced molecule can then be expanded to reveal true parts of the molecule, and cut-outs of the whole molecule can be shown to see only relevant parts of the molecule. In some cases, however, it may be necessary for the application user to see every atom of the molecule at all times. It is therefore application dependent if, and to what extent graph visualisation aspects can be applied to molecule visualisation.

\subsection{General requirements}
\seclab{ms_requirements}
In the earlier discussed study on the use of molecule visualisation software in chemistry education in Finnish secondary education (see \secref{ms_visualisation}), and another study on the user of computer programs in biology~\cite{taylor2013interface}, several requirements for molecule modelling programs have been identified. Molecule modelling software should be easy to use, visually appealing, able to save information, and moderately priced~\cite{aksela2008computer}. Furthermore, it should allow its users to rotate and scale molecules, and have logical ways of interaction. Additionally, it is highly beneficial to have different visual representations of the molecule, or to at least be able to modify the visualisation parameters at runtime. This allows the application user to fully tailor the visualisation to his needs. Finally, a clear and extensive manual is needed that not only describes the basic features of the system, but also the more advanced aspects is great detail.


\section{User studies}
\seclab{user_studies}
In Software Engineering, an often used method to evaluate a project is performing a user study. One type of user studies is the experiment, in which two (or more) different configurations of the examined tool are compared~\cite{wohlin2003empirical}. It is important to define exactly what one wants to validate, which should preferably be something that can easily be observed (e.g. `time required' or `number of clicks', but not `comprehensibility')~\cite{stein2009assessing}. Furthermore, it is important that all different configurations are semantically equivalent (i.e. doing the same thing), have an equal degree of compression (i.e. contain the same information) and are formatted into their cleanest and clearest extent.

% Before starting an experiment, it is very important to make sure the subjects are committed to the tasks they need to perform. Otherwise, the experiment will not properly reflect the production environment. Furthermore, the experiment should be well-prepared, as start-up problems might negatively impact the subject's opinion otherwise.

Another type of user studies is the survey. In a survey, it is easy to test a large number of variables, but one should be careful not to test too many, as this will make the analysis infeasible~\cite{wohlin2003empirical}. In order to collect questionnaire information in a structured way, its questions should follow some natural flow that embodies all aspects of a user's experience~\cite{tuch2013analyzing}. This encourages them to report their detailed experiences, as this is less demanding than answering one big open question. Furthermore, it also reduces the chances of subjects failing to report certain things, simply because they forgot about them.

A variety of predefined surveys is available for usability testing, including \verb|SUS|~(System Usability Scale)~\cite{brooke2013sus} and \verb|UMUX|~(Usability Metric for User Experience)~\cite{lewis2013umux}. \verb|SUS| consists of ten items, each of which should be graded on a scale of 1 (strongly disagree) to 5 (strongly agree). From these items, a total score on a scale of 0-100 can be calculated, with the average system scoring 68 points~\cite{sauro2011measuring}. \verb|SUS| has been extensively tested and has shown to be very reliable, both in general and in comparison with other surveys. It has also been found to deliver correct results even when the number of test subjects is small, with a correctness score of over $90\%$ for a group of only 10 people~\cite{tullis2004comparison}.

The \verb|SUS| questionnaire comes in two variants: one with alternating positive and negative statements, and one with only positive ones. The latter of these has been shown to have a higher reliability, presumably due to the effects of the alternating meaning of (dis)agreeing with a statement, which may confuse the user~\cite{lewis2013umux}. This finding has lead to the development of \verb|UMUX-LITE|, a questionnaire consisting of just the positive statements of \verb|UMUX|. \verb|UMUX-LITE| has been shown to yield results of equal reliability as those of the complete \verb|UMUX|, despite the fact that it consists of just two questions. Even though it is less reliable than \verb|SUS|, it is believed that when a system is subjected to both of these surveys, one can get very reliable results on the usability of that system.

% On a final note about surveys, the fact that they are often straightforward makes them perfectly suitable for being held over the internet. It has been shown that the results of online surveys do not significantly differ from their offline counterparts~\cite{komarov2013crowdsourcing}. However, face-to-face surveys are still the only way that allows both the interviewer and the test subject to immediately ask and answer questions~\cite{wohlin2003empirical}. Nevertheless, the effects of this are considered minimal and can often be mitigated by asking the test subject if he has any questions in the questionnaire.

% Before analysing the data obtained by a user study, it is important to validate it~\cite{wohlin2003empirical}. All the obtained results should be complete and correctly documented. Furthermore, extreme outliers should be identified dealt with. What should happen exactly depends on the situation, but in most cases it is best to leave out the extreme results~\cite{komarov2013crowdsourcing}. Finally, it is very important to provide a grade of validity along with the conclusions of the user studies. This grade depends on, for instance, the number of test subjects, their representativeness or the distribution of the results.
